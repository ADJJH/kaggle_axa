wine.train = wine[index_wine,]
wine.test = wine[-index_wine,]
model_rtree = rpart (quality ~ .,data=wine)
model_rtree
summary(model_rtree)
fancyRpartPlot(model_rtree)
model_svm = ksvm(quality ~ ., data=wine, kernel='vanilladot')
predict_svm = predict(model_svm,wine.test)
head(predict_svm)
summary(predict_svm)
summary(wine$quality)
Loading data:
```{r}
wine = read.csv("R4/Rweek4/whitewines.csv")
#wine$quality = as.factor(wine$quality)
```
Exporing data:
```{r}
summary(wines)
boxplot(log(wines[,c(-7)]))
hist(wines$quality)
```
Creating testing and training sets:
```{r}
set.seed(345)
index_wine = sample (1:nrow(wine),round(nrow(wine)*.8),replace = FALSE)
wine.train = wine[index_wine,]
wine.test = wine[-index_wine,]
```
Building regression tree:
```{r}
model_rtree = rpart (quality ~ .,data=wine)
model_rtree
summary(model_rtree)
fancyRpartPlot(model_rtree)
```
Performance:
```{r}
predict_rtree <- predict(model_rtree, wine.test)
summary(predict_rtree)
summary(wine$quality)
cor(predict_rtree, wine.test$quality)
MAE <- function(actual, predicted) {
mean(abs(actual - predicted))
}
MAE(predict_rtree, wine.test$quality)
```
Model tree:
```{r}
model_m5p <- M5P(quality ~ ., data = wine.train)
summary(model_m5p)
model_m5p
```
Performance:
```{r}
predict_m5p = predict(model_m5p,wine.test)
summary(predict_m5p)
summary(wine$quality)
cor(predict_m5p, wine.test$quality)
MAE(predict_m5p, wine.test$quality)
```
Using the model tree the MAE improves from 0.58 to 0.55, and the correlation from 0.54 to 0.58.
###Exercise 2 - Random Forest
```{r}
model_forest = randomForest(quality ~ .,data=wine.train, ntree= 500)
summary(model_forest)
model_forest
model_forest2 = randomForest(quality ~ .,data=wine.train, ntree= 100)
wine = read.csv("R4/Rweek4/whitewines.csv")
set.seed(345)
index_wine = sample (1:nrow(wine),round(nrow(wine)*.8),replace = FALSE)
wine.train = wine[index_wine,]
wine.test = wine[-index_wine,]
model_svm = ksvm(quality ~ ., data=wine, kernel='vanilladot')
predict_svm = predict(model_svm,wine.test)
head(predict_svm)
summary(predict_svm)
summary(wine$quality)
summary(predict_svm[1])
wine = read.csv("R4/Rweek4/whitewines.csv")
set.seed(345)
index_wine = sample (1:nrow(wine),round(nrow(wine)*.8),replace = FALSE)
wine.train = wine[index_wine,]
wine.test = wine[-index_wine,]
model_svm = ksvm(quality ~ ., data=wine, kernel='vanilladot')
predict_svm = predict(model_svm,wine.test)
head(predict_svm)
summary(predict_svm[1])
summary(wine$quality)
summary(predict_svm)
sum(wine$quality == 9)
sum(wine$quality == 8)
cars = read.csv("car.csv")
View(cars)
summary(cars)
dim(cars)
str(cars)
library(ISLR)
library(ggplot2)
library(gridExtra)
library(psych)
library(class)
library(caret)
library(sampling)
library(e1071)
var_list = names(cars)[1:7]
boxplot_list = list()
for (i in 1:6) {
p = ggplot(data=cars,aes_string(x=var_list[[7]], y=var_list[[i]])) +
geom_boxplot() + ggtitle(i)
boxplot_list[[i]] = p
}
grid.arrange(boxplot_list[[1]],boxplot_list[[2]], boxplot_list[[3]], boxplot_list[[4]], boxplot_list[[5]], boxplot_list[[6]], nrow = 2,ncol=3)
var_list = names(cars)[1:7]
boxplot_list = list()
for (i in 1:6) {
p = ggplot(data=cars,aes_string(x=var_list[[7]], y=var_list[[i]])) +
geom_point() + ggtitle(i)
boxplot_list[[i]] = p
}
grid.arrange(boxplot_list[[1]],boxplot_list[[2]], boxplot_list[[3]], boxplot_list[[4]], boxplot_list[[5]], boxplot_list[[6]], nrow = 2,ncol=3)
plot(cars)
mosaicplot(cars)
mosaicplot(cars$Class,cars$Safety)
class.Doors = table(Class,Doors, data=cars)
class.Doors = table(cars$Class,cars$Doors)
class.Doors
mosaicplot(class.Doors, main = "Class - Doors",
class.doors = table(cars$Class,cars$Doors)
mosaicplot(class.doors, main = "Class - Doors",
xlab = "BMI", ylab = "age", cex = 0.75, color = TRUE)
class.doors = table(cars$Class,cars$Doors)
mosaicplot(class.doors, main = "Class - Doors",xlab = "BMI", ylab = "age", cex = 0.75, color = TRUE)
mosaicplot(class.doors, main = "Class - Doors", cex = 0.75, color = TRUE)
mosaicplot(class.doors, main = "Class - Doors",xlab = "Class", ylab = "Doors", cex = 0.75, color = TRUE)
class.buying = table(cars$Class,cars$Buyig)
mosaicplot(class.buying, main = "Class - Doors",xlab = "Class", ylab = "Buying", cex = 0.75, color = TRUE)
class.buying = table(cars$Class,cars$Buying)
mosaicplot(class.buying, main = "Class - Doors",xlab = "Class", ylab = "Buying", cex = 0.75, color = TRUE)
ggplot(Buying,Maint, data = cars fill=Class) + geom_point()
ggplot(Buying,Maint, data = cars, fill=Class) + geom_point()
ggplot(aes(Buying,Maint, data = cars, fill=Class)) + geom_point()
ggplot(aes(Buying,Doors, data = cars, fill=Class)) + geom_point()
library(ISLR)
library(ggplot2)
library(gridExtra)
library(psych)
library(class)
library(caret)
library(sampling)
library(e1071)
class.doors = table(cars$Class,cars$Doors)
mosaicplot(class.doors, main = "Class - Doors",xlab = "Class", ylab = "Doors", cex = 0.75, color = TRUE)
cars = read.csv("car.csv")
mosaicplot(class.doors, main = "Class - Doors",xlab = "Class", ylab = "Doors", cex = 0.75, color = TRUE)
class.doors = table(cars$Class,cars$Doors)
mosaicplot(class.doors, main = "Class - Doors",xlab = "Class", ylab = "Doors", cex = 0.75, color = TRUE)
summary(cars)
ggplot(aes(Buying,Doors, data = cars, fill=Class)) + geom_point()
class.buying = table(cars$Class,cars$Buying)
mosaicplot(class.buying, main = "Class - Doors",xlab = "Class", ylab = "Buying", cex = 0.75, color = TRUE)
class.maint = table(cars$Class,cars$Maint)
mosaicplot(class.maint, main = "Class - Doors",xlab = "Class", ylab = "Maintenance", cex = 0.75, color = TRUE)
with(cars, ftable(Class ~ Maint + Buying))
class.buy.maint = with(cars, ftable(Class ~ Maint + Buying))
mosaicplot(class.buy.maint, main = "Class - Doors",xlab = "Class", ylab = "Maintenance", cex = 0.75, color = Class)
mosaicplot(class.buy.maint, main = "Class - Doors",xlab = "Class", ylab = "Maintenance", cex = 0.75)
plot(cars$Buying,cars(Maint))
plot(cars$Buying,cars$Maint)
plot(cars$Class,cars$Buying)
plot(cars$Class,cars$Buying, xlab="Class",ylab="Buying")
plot(cars$Class,cars$Buying, xlab="Class",ylab="Buying")
p1 = plot(cars$Class,cars$Buying, xlab="Class",ylab="Buying")
p2 = plot(cars$Class,cars$Maint, xlab="Class",ylab="Maint")
p3 = plot(cars$Class,cars$Doors, xlab="Class",ylab="Doors")
p4 = plot(cars$Class,cars$Persons, xlab="Class",ylab="Persons")
p5 = plot(cars$Class,cars$Lung_boot, xlab="Class",ylab="Lung_boot")
p6 = plot(cars$Class,cars$Safety, xlab="Class",ylab="Safety")
grid.arrange(p1,p2,p3,p4,p5,p6, nrow = 2,ncol=3)
grid.arrange(p1,p2,p3,p4,p5,p6, nrow = 2,ncol=3)
p4 = plot(cars$Class,cars$Persons, xlab="Class",ylab="Persons")
p5 = plot(cars$Class,cars$Lung_boot, xlab="Class",ylab="Lung_boot")
cars$Lung_boot
p5 = plot(cars$Class,cars$Lug_boot, xlab="Class",ylab="Lug_boot")
trainIndex <- createDataPartition(cars, p = .7)
head(trainIndex)
cars.train <- cars[ trainIndex,c(1:6)]
cars.test  <- cars[-trainIndex,c(1:6)]
cars.train_labels <- cars[trainIndex, 7]
cars.test_labels <- cars[-trainIndex, 7]
trainIndex <- createDataPartition(cars, p = .7)
head(trainIndex)
trainIndex <- sample(1:nrow(cars), round(nrow(cars)*.7))
head(trainIndex)
set.seed{123}
set.seed(123)
trainIndex <- sample(1:nrow(cars), round(nrow(cars)*.7))
head(trainIndex)
cars.train <- cars[ trainIndex,c(1:6)]
cars.test  <- cars[-trainIndex,c(1:6)]
cars.train_labels <- cars[trainIndex, 7]
cars.test_labels <- cars[-trainIndex, 7]
model_cars_naive = naiveBayes( cars.train[,-7],cars.train_labels)
predict_cars_naive = predict(model_cars_naive,cars.test[,-7] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,6)],cars.train_labels)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,5,6)],cars.train_labels)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,3,5,6)],cars.train_labels)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,3,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,3,4,5,6)],cars.train_labels)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,3,4,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,4,5,6)],cars.train_labels)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,4,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,4,5,6)],cars.train_labels,laplace =1)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,4,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,4,5,6)],cars.train_labels,laplace =2)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,4,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,4,5,6)],cars.train_labels,laplace = 3)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,4,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
model_cars_naive = naiveBayes( cars.train[,c(1,2,4,5,6)],cars.train_labels,laplace = 0)
predict_cars_naive = predict(model_cars_naive,cars.test[,c(1,2,4,5,6)] , type= "class")
xtab_cars_naive = table(predict_cars_naive,cars.test_labels)
confusionMatrix(xtab_cars_naive)
library(caret)
library(pROC)
data(iris)
iris <- iris[iris$Species == "virginica" | iris$Species == "versicolor", ]
iris$Species <- factor(iris$Species)  # setosa should be removed from factor
samples <- sample(NROW(iris), NROW(iris) * .5)
data.train <- iris[samples, ]
data.test <- iris[-samples, ]
forest.model <- train(Species ~., data.train)
result.predicted.prob <- predict(forest.model, data.test, type="prob") # Prediction
result.roc <- roc(data.test$Species, result.predicted.prob$versicolor)
result.predicted.prob
class(result.predicted.prob)
forest.model
result.predicted.pro
b
result.predicted.prob
forest.model
head(data.test)
class(forest.model)
class(data.test)
readRDS("Models/fit.rds")
setwd("~/BC files/R ex/Kaggle/kaggle_axa")
readRDS("Models/fit.rds")
fit=readRDS("Models/fit.rds")
fit$"OOB-AUCopt
fit$"OOB-AUCopt"
fit.cv = AUCRFcv(fit,
nCV = 10, ###Number of folds.
M = 10)
fit.cv = AUCRFcv(fit,
nCV = 10, ###Number of folds.
M = 10)
library(ggplot2)
library(GGally)
library(corrplot)
library(stats)
library(VIM)
library(AUCRF)
load("Data/small_axa.RData")
converted.columns = sapply(axa[, 2:87], as.numeric)
axa.converted = data.frame(axa[, 1], converted.columns, axa[, 88])
names(axa.converted) = names(axa)
axa.converted$driver = as.factor(substr(axa.converted$driver_trip, 1, 1))
names <- names(axa.converted)
names <- gsub("-", "_", names)
names(axa.converted) <- names
remove(axa)
axa = axa.converted
remove(axa.converted)
axa.new = axa[, -c(1, 74:79, 90)]
axa.knn = kNN(axa.new)
axa.imputed = axa.knn[, 1:82]
axa.imputed$isDriver = as.factor(axa.imputed$isDriver)
levels(axa.imputed$isDriver) = c(0,1)
fit = readRDS("Models/fit.rds")
fit.cv = AUCRFcv(fit,
nCV = 10, ###Number of folds.
M = 10)
fit.cv$cvAUC
fit.cv = readRDS("Models/fit.cv.rds")
saveRDS(fit.cv,"Models/fit.cv.rds")
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 6) #AUC 0.8314 - 11Kopt,
saveRDS(fit_wo_driver_nodesize,"Models/fit_wo_driver_nodesize.rds")
fit_wo_driver_nodesize$Kopt
fit_wo_driver_nodesize$"OOB-AUCopt"
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 6)
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 6) #AUC 0.8368 - 11Kopt,
summary(fit_wo_driver_nodesize)
plot(fit_wo_driver_nodesize)
View(axa.imputed)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7)
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_nodesize = AUCRF(isDriver ~ .,
ntree = 1000,
nodesize = 7,
data = axa.imputed) #.8837
plot(fit_nodesize)
set.seed(12345)
fit_nodesize = AUCRF(isDriver ~ .,
ntree = 1000,
nodesize = 10,
data = axa.imputed) #.8837
plot(fit_nodesize)
set.seed(12345)
fit_nodesize = AUCRF(isDriver ~ .,
ntree = 1000,
nodesize = 6,
data = axa.imputed) #.8837
plot(fit_nodesize)
summary(fit_nodesize)
summary(fit_nodesize)
fit.cv.nodesize = AUCRFcv(fit_nodesize,
nCV = 10, ###Number of folds.
M = 10)
plot(fit.cv.nodesize)
saveRDS(fit.cv.nodesize,"Models/fit.cv.nodesize.rds")
fit.cv.nodesize = readRDS("Models/fit.cv.nodesize.rds")
summary(fit.cv.nodesize) # auc =0.857, 11 kopt
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
set.seed(12345)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
saveRDS(fit_wo_driver_nodesize,"Models/fit_wo_driver_nodesize.rds")
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.8368 - 11Kopt,
summary(fit_wo_driver_nodesize)
plot(fit_wo_driver_nodesize)
fit_wo_driver_nodesize= AUCRF(isDriver ~ .,
ntree = 1000,
data = axa.imputed[-82],
nodesize = 7) #AUC 0.83 - 19Kopt,
plot(fit_wo_driver_nodesize)
saveRDS(fit_wo_driver_nodesize,"Models/fit_wo_driver_nodesize.rds")
fit.cv_wo_driver_nodesize = AUCRFcv(fit_wo_driver_nodesize,
nCV = 10, ###Number of folds.
M = 10) ###Number of CV repetitions.
summary(fit.cv_wo_driver_nodesize)
getwd()
cor_axa_t[which.min(cor_axa_t)]
which.min(cor_axa_t)
correlation_axa = cor(axa[,79] , axa[,2:87] , use = "pairwise.complete.obs")
cor_axa_t = t(correlation_axa)
which.min(cor_axa_t)
which.max(cor_axa_t[-78,])
cor_axa_t[which.min(cor_axa_t)]
cor_axa_t[which.min(cor_axa_t),]
cor_axa_t[which.min(cor_axa_t),1]
typecor_axa_t
type(cor_axa_t)
class(cor_axa_t)
class(cor_axa_t)[1][1]
cor_axa_t[1][1]
cor_axa_t[1]
View(cor_axa_t)
cor_axa_t[which.min(cor_axa_t),1]
which.min(cor_axa_t)
which.min(cor_axa_t[1])
row.names(cor_axa_t[which.min(cor_axa_t),1])
row.names(cor_axa_t[which.min(cor_axa_t)])
row.names(cor_axa_t[2])
row.names(cor_axa_t[2],)
row.names(cor_axa_t[2,])
a=which.min(cor_axa_t)
a
a=2
a
row.names(cor_axa_t)
row.names(cor_axa_t[1])
row.names(cor_axa_t[[1]])
row.names(cor_axa_t[1])
row.names(cor_axa_t)[1]
row.names(cor_axa_t)[which.min(cor_axa_t)]
fit_nodesize = readRDS("Models/fit_nodesize.rds")
set.seed(12345)
fit_nodesize = AUCRF(isDriver ~ .,
ntree = 1000,
nodesize = 6,
data = axa.imputed) #.8809
set.seed(12345)
fit_nodesize = AUCRF(isDriver ~ .,
ntree = 1000,
nodesize = 6,
data = axa.imputed) #.8809
plot(fit_nodesize) # 11
saveRDS(fit_nodesize,"Models/fit_nodesize.rds")
saveRDS(fit_wo_driver_nodesize,"Models/fit_wo_driver_nodesize.rds")
plot(fit_wo_driver_nodesize)
plot(fit.cv_wo_driver_nodesize)
plot(fit.cv_wo_driver_nodesize)
names(fit.cv_wo_driver_nodesize)
saveRDS(fit.cv_wo_driver_nodesize,"Models/fit.cv_wo_driver_nodesize.rds")
fit.cv_wo_driver_nodesize = readRDS("Models/fit.cv_wo_driver_nodesize.rds")
r fit.cv$ranking[1:15]
fit.cv$ranking[1:15]
class(r fit.cv$ranking[1:15])
class(fit.cv$ranking[1:15])
class(fit.cv$ranking)
fit.cv$ranking
fit.cv$ranking[1:15,]
colnames(fit.cv$ranking[1:15])
rownames(fit.cv$ranking[1:15])
rownames(fit.cv$ranking)
fit.cv$ranking
fit.cv$ranking[1]
fit.cv$ranking[1,1]
row.names(fit.cv$ranking[1,1])
row.names(fit.cv$ranking[1])
names(fit.cv$ranking[1])
