---
title: "Kaggle Axa Project"
author: "Alejandra Jaimes, Alex Signal, Christopher Peter Makris, Dani Ismail"
date: "July 2015"
output: html_document
---



```{r echo=TRUE, results='hide',message=FALSE}
library(ggplot2)
library(GGally)
library(corrplot)
```


###Loading Data
```{r}
load("Data/small_axa.RData")
```

### Data preparation
Preparing data frame by converting variables to numeric. 
```{r}
dim(axa) #600 Observations, 88 Variables
names(axa)
sapply(axa, class)
table(sapply(axa, class)) #Characters need to be converted to numeric type.

###Converting character variables to numeric variables.
converted.columns = sapply(axa[, 2:87], as.numeric)
axa.converted = data.frame(axa[, 1], converted.columns, axa[, 88])
names(axa.converted) = names(axa) 

axa.converted$driver = as.factor(substr(axa.converted$driver_trip, 1, 1))
names <- names(axa.converted)
names <- gsub("-", "_", names)
names(axa.converted) <- names
remove(axa)
axa = axa.converted
remove(axa.converted)
```

### Data Exploration

```{r}
summary(axa)
```


Checking for NA values. 
```{r}
sum(is.na(axa)) #3,375 missing values.
na.list = as.data.frame(sapply(axa, function(x) { sum(is.na(x)) })) #Focused in certain variables.
na.list$variable = rownames(na.list)
colnames(na.list) = c("sum.na","variable")
rownames(na.list) <- NULL
na.list[order(na.list$sum.na, decreasing = TRUE), ]
```


Creating boxplots split by driver type and response.
```{r}

var_list = names(axa)[1:89]
boxplot_list = list("null")

for (i in 2:88) {
    p = ggplot(data=axa,aes_string(x=var_list[[89]], y=var_list[[i]],
                                   fill= var_list[[88]])) + 
          geom_boxplot() + ggtitle(i)                   
    boxplot_list[[i]] = p
    }
boxplot_list [[79]]
```


Creating plot
```{r, eval = FALSE} 
#change eval = TRUE to create this plot. It requires a lot of computation.
axa2 = axa
axa2$isDriver = as.factor(axa2$isDriver)
ggpairs(data=axa2[,c(3, 4, 5, 7, 8, 18, 23, 24, 28, 32, 34, 48, 74, 75, 76, 77, 78, 79, 84, 85,88)], color ="isDriver" )
```


From the boxplots, variable `r  colnames(axa[79])` show high variances when comparing different drivers and the reponse. 

However this variable has 552/600 empty values. Since we consider this can be a variable that could predict the reponse. We decided to impute ths variable and check if we can use it in our model. 

Checking the correlation with variable 79 (`r  colnames(axa[79])`) and selecting the max and min correlations. 
```{r}
correlation_axa = cor(axa[,79] , axa[,2:87] , use = "pairwise.complete.obs")
cor_axa_t = t(correlation_axa)
which.min(cor_axa_t)
which.max(cor_axa_t[-78,])

```
The variables with the highest correaltions are: `r which.min(cor_axa_t)` and `rwhich.max(cor_axa_t[-78,])`. 

We picked variable `r which.min(cor_axa_t)` with the highest correlation with variable 79.

Using linear regression to impute the values. 

First model using variable accel.quantile.0 variable to predict attr.hv.sd 
```{r}
model = lm(attr.hv.sd ~ accel_quantile.0., data = axa)
summary(model)
par(mfrow=c(2, 2))
plot(model)
```
From the "fitted vs residuals values"" plot a pattern is observed. 

Checking for transformations:

```{r}
library(MASS)
bc = boxcox(model)
which(bc$y == max(bc$y))
bc$x[54]
```
The boxcox plot shows that we can apply a transformation to remove the pattern. 

Transforming the data by taking the max point of the boxcox transformation, and using it in a new imputing model:

```{r}
transformed = axa$attr.hv.sd^bc$x[54]
model2 = lm(transformed ~ accel_quantile.0., data = axa)

par(mfrow=c(2, 2))
plot(model2)

col.vec = rep(1, nrow(axa))
col.vec[305] = "red3"
plot(transformed ~ axa$accel_quantile.0., col = col.vec)

```
From model2, a leverage point (observation 305) is analyzed.

In the following model the levarage point is removed from the imputing model.
```{r}
acceleration = axa$accel_quantile.0.[-305]

model3 = lm(transformed[-305] ~ acceleration, data = axa)
par(mfrow=c(2, 2))
plot(model3)

summary(model3)
```
The R-squared value after removing the levarage improved from `r summary(model2)$adj.r.squared` to `r summary(model3)$adj.r.squared`

Model3 is now used to impute the missing values.

```{r}
input = axa$accel_quantile.0.[is.na(axa$attr.hv.sd)] #Selecting predictor values 
#with attr.hv.sd = NA. 

index = which(is.na(axa$attr.hv.sd)) 
imputed.values = predict(model3,  newdata = data.frame(acceleration=input))

axa$attr.hv.sd.imputed = axa$attr.hv.sd
axa[index,"attr.hv.sd.imputed"]  = imputed.values
```

```{r}
library (ggplot2)
library(rpart)
library(rattle)

axa = axa[,-c(1,74:79,90)]
set.seed(345)
index_axa = sample (1:nrow(axa),round(nrow(axa)*.8),replace = FALSE)
axa.train = axa[index_axa,]
axa.test = axa[-index_axa,]

model_rtree = rpart (isDriver ~ .,data=axa)
fancyRpartPlot(model_rtree)
#model_forest = randomForest(quality ~ .,data=wine.train, ntree= 500)

```


```{r}
library(VIM)
library (caret)
library(pROC)
axa.new = axa[, -c(1, 74:79, 90)]
axa.knn = kNN(axa.new)
axa.imputed = axa.knn[, 1:82]
axa.imputed$isDriver = as.factor(axa.imputed$isDriver)

set.seed(998)
inTraining <- createDataPartition(axa.imputed$isDriver, p = .75, list = FALSE)
training <- axa.imputed[ inTraining,]
testing  <- axa.imputed[-inTraining,]


rf_model<-train(isDriver ~ . ,
                data=training,
                method="rf",
                trControl=trainControl(method="cv",number=5))
#,
                #metric = "ROC")
print(rf_model)
#

rf.model2 <-randomForest(isDriver ~.,data= training, mtry=2, ntree=100, 
     keep.forest=TRUE, importance=TRUE,test=testing)


predicted.prob <- predict(rf.model2,testing, type="prob") # Prediction
colnames(predicted.prob) <- c("not_driver","driver")

result.roc <- roc(testing$isDriver, predicted.prob$driver) # Draw ROC curve.
plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")

result.coords <- coords(result.roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
print(result.coords)#to get threshold and accuracy

```








